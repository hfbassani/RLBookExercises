{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex_5.09.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExfhortJQgW"
      },
      "source": [
        "# Disciplina: Aprendizagem por Reforço\n",
        "**Aluno:** Julio Cezar Soares Silva\n",
        "\n",
        "**Professor:** Hansenclever Bassani \n",
        "\n",
        "**Exercício 5.09**\n",
        "Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to\n",
        "use the incremental implementation for sample averages described in Section 2.4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zTVJlOr1Dv6"
      },
      "source": [
        "=============================================================================\n",
        "\n",
        "Algoritmo - Off-policy MC prediction for esimating $Q \\approx q_\\pi$\n",
        "\n",
        "Input: Target policy $\\pi$\n",
        "\n",
        "1 Initialize $Q(s,a)$ arbitrarily\n",
        "\n",
        "2 $C(s,a) \\leftarrow 0$\n",
        "\n",
        "3 Loop forever (for each episode):\n",
        "\n",
        "--- 3.1 $b \\leftarrow$ any policy with coverage of $\\pi$\n",
        "\n",
        "--- 3.2 $EP_{b} = [S_0, A_0, R_1, ... S_{T-1}, A_{T-1}, R_{T}]$ \n",
        "\n",
        "--- 3.3 $G \\leftarrow 0$ \n",
        "\n",
        "--- 3.4 $W \\leftarrow 1$ \n",
        "\n",
        "--- 3.5 Loop for each step $t$ of $EP_b$, $t = T-1, T-2, ..., 0$\n",
        "\n",
        "--- --- 3.5.1 $G \\leftarrow \\gamma G + R_{t+1}$ \n",
        "\n",
        "--- --- 3.5.2 $C(S_t, A_t) \\leftarrow C(S_t, A_t) + W$ \n",
        "\n",
        "--- --- 3.5.3 $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{W}{C(S_t,A_t)}\\big[G - Q(S_t,A_t)\\big]$ \n",
        "\n",
        "--- --- 3.5.4 $W \\leftarrow W\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$\n",
        "\n",
        "=============================================================================\n",
        "\n",
        "Para converter um algoritmo de every-visit para first-visit, é necessário verificar se é a primeira visita a um determinado par estado-ação $(S_t, A_t)$ antes de atualizar $Q(S_t,A_t)$ em um determinado episódio.\n",
        "\n",
        "=============================================================================\n",
        "\n",
        "Algoritmo - First-visit off-policy MC prediction for esimating $Q \\approx q_\\pi$\n",
        "\n",
        "Input: Target policy $\\pi$\n",
        "\n",
        "1 Initialize $Q(s,a)$ arbitrarily\n",
        "\n",
        "2 $C(s,a) \\leftarrow 0$\n",
        "\n",
        "3 Loop forever (for each episode):\n",
        "\n",
        "--- 3.1 $b \\leftarrow$ any policy with coverage of $\\pi$\n",
        "\n",
        "--- 3.2 $EP_{b} = [S_0, A_0, R_1, ... S_{T-1}, A_{T-1}, R_{T}]$ \n",
        "\n",
        "--- 3.3 $G \\leftarrow 0$ \n",
        "\n",
        "--- 3.4 $W \\leftarrow 1$ \n",
        "\n",
        "--- 3.5 Loop for each step $t$ of $EP_b$, $t = T-1, T-2, ..., 0$\n",
        "\n",
        "--- --- 3.5.1 $G \\leftarrow \\gamma G + R_{t+1}$ \n",
        "\n",
        "--- --- 3.5.2 $C(S_t, A_t) \\leftarrow C(S_t, A_t) + W$ \n",
        "\n",
        "--- --- 3.5.3 IF $(S_t,A_t) \\notin \\{(S_0,A_0), (S_1,A_1), ...,(S_{t-1},A_{t-1})\\}$\n",
        "\n",
        "--- --- --- 3.5.3.1 $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{W}{C(S_t,A_t)}\\big[G - Q(S_t,A_t)\\big]$ \n",
        "\n",
        "--- --- 3.5.4 $W \\leftarrow W\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$\n",
        "\n",
        "=============================================================================\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}