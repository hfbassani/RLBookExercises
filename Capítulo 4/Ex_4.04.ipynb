{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex4_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExfhortJQgW"
      },
      "source": [
        "# Disciplina: Aprendizagem por Reforço\n",
        "**Aluno:** Julio Cezar Soares Silva\n",
        "\n",
        "**Professor:** Hansenclever Bassani \n",
        "\n",
        "**Exercício 4.4**\n",
        "The policy iteration algorithm on page 80 has a subtle bug in that it may\n",
        "never terminate if the policy continually switches between two or more policies that are\n",
        "equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so\n",
        "that convergence is guaranteed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAKba4e9FIrC"
      },
      "source": [
        "=============================================================================\n",
        "\n",
        "Algoritmo - Policy Improvement (Original com bug)\n",
        "\n",
        "1 $policyStable = true$\n",
        "\n",
        "2 For each $s \\in S$:\n",
        "\n",
        "--- 2.1 $oldAction = \\pi(s)$\n",
        "\n",
        "--- 2.2 $\\pi(s) = arg\\max\\limits_{a}\\sum_{s',r}p(s',r|s,a)\\big[r+\\gamma V(s')\\big]$\n",
        "\n",
        "--- 2.3 If $oldAction \\neq \\pi(s)$, then $policyStable = false$\n",
        "\n",
        "3 If $policyStable$, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to Policy Evaluation\n",
        "\n",
        "=============================================================================\n",
        "\n",
        "Para que o algoritmo convirja, é necessário modificar a condição If $oldAction \\neq \\pi(s)$, then $policyStable = false$, de modo que possamos avaliar apenas o valor máximo alcançado por determinada política e não continuar a verificar se as ações são diferentes. \n",
        "\n",
        "=============================================================================\n",
        "\n",
        "Algoritmo - Policy Improvement (Solução proposta)\n",
        "\n",
        "1 $policyStable = true$\n",
        "\n",
        "2 For each $s \\in S$:\n",
        "\n",
        "--- 2.1 $oldAction = \\pi(s)$\n",
        "\n",
        "--- 2.2 $oldVal = \\sum_{s',r}p(s',r|s,oldAction)\\big[r+\\gamma V(s')\\big]$\n",
        "\n",
        "--- 2.3 $\\pi(s) = arg\\max\\limits_{a}\\sum_{s',r}p(s',r|s,a)\\big[r+\\gamma V(s')\\big]$\n",
        "\n",
        "--- 2.4 If $oldVal < \\sum_{s',r}p(s',r|s,\\pi(s)))\\big[r+\\gamma V(s')\\big]$, then $policyStable = false$\n",
        "\n",
        "3 If $policyStable$, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to Policy Evaluation\n",
        "\n",
        "=============================================================================\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}