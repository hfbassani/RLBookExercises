{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex_6.07.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExfhortJQgW"
      },
      "source": [
        "# Disciplina: Aprendizagem por Reforço\n",
        "**Aluno:** Julio Cezar Soares Silva\n",
        "\n",
        "**Professor:** Hansenclever Bassani \n",
        "\n",
        "**Exercício 6.07**\n",
        "Design an on-policy version of the TD(0) update that can be used with arbitrary target policy $\\pi$ and covering behavior policy b, using at each step t the importance sampling ratio $\\rho_{t:t}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCPtuMkb1iVH"
      },
      "source": [
        "Considerando weighted importance sampling, temos que:\n",
        "\n",
        "$$\\rho_{t:t} = \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} $$\n",
        "\n",
        "Em TD, temos que $G_{t:t+1} = R_{t+1} + \\gamma Q(S_{t+1},A_{t+1})$. O incremento se dá por:\n",
        "\n",
        "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha\\rho_{t:t}\\big[R_{t+1} +  \\gamma Q(S_{t+1},A_{t+1}) - Q(S,A)\\big]$$"
      ]
    }
  ]
}