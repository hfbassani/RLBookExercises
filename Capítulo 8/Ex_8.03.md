### *Exercise 8.3:*

**Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?**

---
Resposta 1:

```
O Dyna-Q+ encontra de uma maneira mais r√°pida a pol√≠tica √≥tima pela sua maior natureza explorat√≥ria, por√©m mesmo j√° possuindo a pol√≠tica √≥tima ele √© for√ßado a continuar explorando, √© por esse custo explorat√≥rio que a diferen√ßa entre Dyna-Q + e Dyna-Q vai se estreitando
```

---
Resposta 2:

```
Durante a primeira parte do experimento, rapidamente o Dyna-Q+ encontra uma boa pol√≠tica por conta dos passos explorat√≥rios reaproveitando a experi√™ncia anterior. Por√©m, o Dyna-Q acaba se aproximando do Dyna-Q+ justamente por conta dos passos aleat√≥rios dados pela pol√≠tica epsilon-greedy. Dessa forma, a melhoria no Dyna-Q se d√° pelo passo de aprendizado direto, quando uma a√ß√£o aleat√≥ria √© escolhida. J√° a pequena piora do Dyna-Q+ ocorre pelo fato de que ainda por cima da pol√≠tica epsilon-greedy, o Dyna-Q+ ainda precisa revisitar a√ß√µes que n√£o foram visitadas faz tempo por conta do termo k * ‚àöùúè. Perceba que a ordem de decaimento do tempo ùúè dessas a√ß√µes segue a visita√ß√£o original das a√ß√µes, isto √©, se eu visito os estados S1, S2, S3, ent√£o S1 "decair√°" primeiro de forma a ter um alto k * ‚àöùúè, logo em seguida S2 e assim por diante. Como o passo de planning acontece mais vezes, mas ent√£o o aspecto greedy da pol√≠tica original se transfere e se soma na fase de planejamento, decaindo o Dyna-Q+ em performance um pouco mais do que o Dyna-Q. Se o uso de uma pol√≠tica greedy faz com que a acur√°cia m√°xima do agente seja 90%, ent√£o com o Dyna-Q+ ela acaba sendo um pouco menor que 90%.
```