{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Ex_4.05.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"U21P1stQ0oNo"},"source":["#### *Exercise 4.5*\n","\n","#### How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book."]},{"cell_type":"markdown","metadata":{"id":"TAr0M8ag0oNw"},"source":["---\n","#### Resposta 1:\n","\n","1. Inicialize $Q(s,a) \\in \\mathbb{R}$ e $\\pi(s) \\in A(s)$ arbitrariamente para todo $s \\in S$, $a \\in A$.\n","\n","2. Policy Evaluation\n","\n","Loop:\n","\n","$\\quad$ $\\Delta$ <-- $0$\n","\n","$\\quad$ Loop para cada $s \\in S$ e $a \\in A(s)$:\n","\n","$\\quad$ $\\quad$ $q$ <-- $Q(s,a)$\n","\n","$\\quad$ $\\quad$ $Q(s,a)$ <-- $ \\sum_{s', r}{p(s',r|s,a)[r + \\gamma\\sum_{a'}{\\pi(a'|s')Q(s',a')} ]} $\n","\n","$\\quad$ $\\quad$ $\\Delta$ <-- $max(\\Delta,|q - Q(s,a)|)$\n","\n","até $\\Delta < \\theta$ (um pequeno número positivo determinando a acurácia da estimação)\n","\n","3. Policy improvement\n","\n","policy-stable <-- true\n","\n","Para cada $s \\in S$:\n","\n","$\\quad$ old-action <-- $\\pi(s)$\n","\n","$\\quad$ $\\pi(s)$ <-- $argmax  Q(s,a)$\n","\n","$\\quad$ Se old-action ≠ $\\pi(s)$, então pplicy-stable <-- false\n","\n","Se policy-stable for true, então pare e retorne $Q$ ≃ $q_*$ e $\\pi$ ≃ $\\pi_*$;\n","\n","Do contrário, vá para 2.\n","\n","A expressão $ \\sum_{s', r}{p(s',r|s,a)[r + \\gamma\\sum_{a'}{\\pi(a'|s')Q(s',a')} ]} $ foi obtida reescrevendo a equação de Bellman (4.2). Semelhante a (4.4) em relação a (4.1), que ao invés de calcular o valor máximo, substitui por $ \\sum_{a}{\\pi(a,s)}$. No caso considerado, ao invés de calcular o valor máximo da função ação valor, calcula-se o $ \\sum_{a}{\\pi(a|s)}$. A partir daí calcula-se $q$ iterativamente, semelhante a $v_{k+1}$ em (4.5)."]}]}