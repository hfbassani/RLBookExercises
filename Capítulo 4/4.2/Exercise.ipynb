{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is vπ(15) for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is vπ(15) for the equiprobable random policy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levei em consideração k = ∞, logo, retirei do livro a tabela com os últimos valores para a função de valor V. Também, utilizei a equação de Bellman para vπ, equação 4.4 do livro. Para os estados subsequentes (12, 13, 14, 15), a partir do novo estado 15, temos os seguintes valores para V:\n",
    "    - V(9) = -20\n",
    "    - V(12) = -22\n",
    "    - V(13) = -20\n",
    "    - V(14) = -14\n",
    "    - V(15) = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) As ações a partir dos estados originais (12, 13 e 14) não mudam:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Levando em consideração a equação de Bellman, temos a seguinte equação, após substituir os valores:\n",
    "    V(15) = 0.25 * (-1 + V(12)) + 0.25 * (-1 + V(13)) + 0.25 * (-1 + V(14)) + 0.25 * (-1 + V(15))\n",
    "    V(15) - 0.25 * V(15) = 0.25 * (-1 + V(12)) + 0.25 * (-1 + V(13)) + 0.25 * (-1 + V(14)) + -0.25\n",
    "    V(15) = (0.25 * (-1 + V(12)) + 0.25 * (-1 + V(13)) + 0.25 * (-1 + V(14)) + -0.25) / 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_equation(policy, reward, value):\n",
    "    value_15 = 0\n",
    "    for i, v in enumerate(value):\n",
    "        value_15 += policy[i] * (reward + v)\n",
    "    value_15 /= 0.75\n",
    "    return value_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bellman_equation([0.25, 0.25, 0.25, 0.25], -1, [-22, -20, -14, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) As ações a partir do estados 13 mudam, indo para o estado 15 quando for down:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Levando em consideração a equação de Bellman, e que o V(15) = -20, podemos verificar o seguinte:\n",
    "    V(13) = 0.25 * (-1 + V(12)) + 0.25 * (-1 + V(14)) + 0.25 * (-1 + V(15)) + 0.25 * (-1 + V(9))\n",
    "    V(13) = -1 + 0.25 * (V(9) + V(12) + V(14) + V(15))\n",
    "    V(13) = -1 + 0.25 * (-20 - 22 - 14 - 20)\n",
    "    V(13) = -1 + 0.25 * (-76)\n",
    "    V(13) = -80 / 4\n",
    "    V(13) = -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ou seja, o V(13) não foi alterado, partindo dessa premissa, o valor para o estado 15, V(15), também não será alterado, logo V(15) = -20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
