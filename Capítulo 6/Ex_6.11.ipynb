{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex_6.11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExfhortJQgW"
      },
      "source": [
        "# Disciplina: Aprendizagem por Reforço\n",
        "**Aluno:** Julio Cezar Soares Silva\n",
        "\n",
        "**Professor:** Hansenclever Bassani \n",
        "\n",
        "**Exercício 6.11**\n",
        "Why is Q-learning considered an on-policy control method? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTmDh-gdDx8a"
      },
      "source": [
        "Primeiro, vamos definir o que é política comportamental e o que é política alvo. A política comportamental objetiva gerar dados ou pares estado-ação que serão avaliados em cada passo do episódio. Em outras palavras, a política comportamental é quem realiza a exploração no espaço de pares estado-ação. A política alvo é aquela que se deseja avaliar e incrementar. \n",
        "\n",
        "Abordagens on-policy aprendem a função qualidade da ação para a política que está sendo utilizada para explorar. Com a abordagem on-policy encontra-se a melhor política que continua sendo exploratória. Uma abordagem on-policy é um caso especial da off-policy quando a política comportamental é igual à política alvo. Vamos observar o algoritmo do SARSA:\n",
        "\n",
        "1 Loop for each episode:\n",
        "\n",
        "--- 1.1 Initialize $S$ \n",
        "\n",
        "--- 1.2 Choose $A$ from $S$ using the policy derived from Q (e.g., $\\epsilon$-greedy)\n",
        "\n",
        "--- 1.3 Loop for each step of this episode\n",
        "\n",
        "--- --- 1.3.1 Observe $R$,  $S'$\n",
        "\n",
        "--- --- **1.3.2** Choose $A'$ from $S'$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy) \n",
        "\n",
        "--- --- **1.3.3** $Q(S,A) \\leftarrow Q(S,A) + \\alpha\\big[R + \\gamma Q(S',A') - Q(S,A)\\big]$ \n",
        "\n",
        "--- --- 1.3.4 $S' \\leftarrow S; A \\leftarrow A';$\n",
        "\n",
        "--- --- Until $S$ is terminal\n",
        "\n",
        "No passo 1.3.2 selecionamos a ação $A'$ em $S'$ a partir de uma política comportamental, portanto iremos avaliar esta política no passo 1.3.3. Essa mesma ação $A'$ escolhida pela política comportamental será utilizada para explorar outro par estado-ação $(A,S)$ no próximo passo. Percebemos que a política exploratória é a política alvo.\n",
        "\n",
        "Métodos off-policy não se importam com a política exploratória, apenas se importam com a atualização contínua dos pares estado-ação para avaliar uma política alvo. Portanto no Q-learning, o agente explora com uma política comportamental, mas quer aprender uma política alvo. No caso do Q-learning, a política alvo é ótima determinística (gananciosa). Vamos observar o algoritmo do Q-learning:\n",
        "\n",
        "1 Loop for each episode:\n",
        "\n",
        "--- 1.1 Initialize $S$ \n",
        "\n",
        "--- 1.2 Loop for each step of this episode\n",
        "\n",
        "--- --- **1.2.1** Choose $A$ from $S$ using the policy derived from Q (e.g., $\\epsilon$-greedy)\n",
        "\n",
        "--- --- 1.2.2 Observe $R$,  $S'$\n",
        "\n",
        "--- --- **1.2.3** $Q(S,A) \\leftarrow Q(S,A) + \\alpha\\big[R + \\gamma \\max\\limits_{a}Q(S',a) - Q(S,A)\\big]$ \n",
        "\n",
        "--- --- 1.2.4 $S' \\leftarrow S;$\n",
        "\n",
        "--- --- Until $S$ is terminal\n",
        "\n",
        "Percebe-se que a política alvo avaliada não explora, apenas escolhe as ações mais gananciosas em cada estado. Portanto, existem duas políticas: uma comportamental (exploratória), que escolhe os próximos pares estado-ação a serem atualizados no passo 1.2.1, e uma gananciosa (alvo) que é avaliada em cada par estado-ação escolhido pela política comportamental no passo 1.2.3."
      ]
    }
  ]
}